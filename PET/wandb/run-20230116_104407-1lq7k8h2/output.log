01/16/2023 10:44:07 - INFO - tools.logger -   Begin Loading the model...
01/16/2023 10:44:31 - INFO - tools.logger -   Model to CUDA.
/azure/yingxiu/ENVS/pet/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
01/16/2023 10:44:37 - WARNING - datasets.builder -   Found cached dataset super_glue (/home/v-yingxzhao/.cache/huggingface/datasets/super_glue/rte/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed)
  0% 0/3 [00:00<?, ?it/s]100% 3/3 [00:00<00:00, 828.59it/s]
01/16/2023 10:44:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/v-yingxzhao/.cache/huggingface/datasets/super_glue/rte/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-c76dd863bb04e287.arrow
01/16/2023 10:44:37 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /home/v-yingxzhao/.cache/huggingface/datasets/super_glue/rte/1.0.3/bb9675f958ebfee0d5d6dc5476fafe38c79123727a7258d515c450873dbdbbed/cache-709e1f0633d8e358.arrow
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 81 of the training set: {'input_ids': [37, 16877, 7, 16, 48, 167, 1100, 3237, 24236, 24, 5531, 6, 1915, 40, 51, 5108, 6, 13962, 10484, 52, 348, 11, 13870, 21702, 29105, 15, 26, 16, 1282, 3251, 12, 23808, 11, 1817, 18591, 342, 981, 3984, 16, 5531, 31, 7, 3409, 7, 7604, 12, 11469, 5531, 3159, 16, 6260, 5, 180, 11160, 31, 7, 13297, 3035, 1381, 165, 1798, 1661, 2737, 377, 5, 1838, 8079, 15, 53, 12, 9278, 6, 7548, 263, 12, 3259, 11, 20855, 10213, 6, 1838, 31, 7, 996, 10569, 11, 72, 5, 86, 11979, 180, 11160, 5132, 3, 9, 6422, 2108, 9953, 16, 84, 34, 7760, 576, 18, 13238, 2009, 13, 66, 13, 11469, 5531, 31, 7, 3409, 7, 21, 13927, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 566, 1092, 1208, 10213, 19, 3259, 31, 7, 4806, 535, 58, 2163, 42, 150, 58, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [465, 1]}.
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 14 of the training set: {'input_ids': [7798, 4027, 107, 51, 6, 8, 25829, 20134, 6, 3, 17, 13296, 21, 220, 3651, 6460, 11, 874, 19396, 7, 38, 8, 21967, 7, 3853, 3644, 8687, 1015, 3, 3891, 18, 2555, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 634, 20134, 3, 17, 13296, 21, 314, 2368, 6460, 11, 386, 19396, 7, 6, 11, 258, 4037, 12, 8, 414, 2901, 192, 72, 648, 535, 58, 2163, 42, 150, 58, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [465, 1]}.
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 3 of the training set: {'input_ids': [3736, 2498, 1813, 5907, 29, 6, 5752, 4297, 44, 749, 15789, 9, 6, 3, 9, 1035, 313, 349, 24, 1691, 12666, 8, 8401, 1201, 18, 1490, 8940, 6219, 2548, 16, 1546, 2695, 4765, 107, 896, 41, 14034, 1138, 23, 5307, 201, 243, 24, 78, 623, 81, 1914, 2560, 502, 43, 1204, 1058, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 634, 1767, 564, 13, 1546, 2695, 4765, 107, 896, 47, 1138, 23, 5307, 535, 58, 2163, 42, 150, 58, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2163, 1]}.
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 94 of the evaluation set: {'input_ids': [[325, 2709, 172, 6, 604, 932, 3, 3914, 3, 18, 325, 2709, 172, 1775, 5076, 5779, 43, 19972, 24, 17032, 139, 8, 7738, 13, 192, 1021, 412, 5, 134, 5, 5169, 33, 271, 4468, 57, 3, 9, 3, 8689, 563, 23655, 15, 26, 12273, 12, 17741, 48, 5447, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 382, 210, 32, 1021, 412, 5, 134, 5, 5169, 130, 4792, 30, 604, 932, 3, 3914, 535, 58, 2163, 42, 150, 58, 1], [325, 2709, 172, 6, 604, 932, 3, 3914, 3, 18, 325, 2709, 172, 1775, 5076, 5779, 43, 19972, 24, 17032, 139, 8, 7738, 13, 192, 1021, 412, 5, 134, 5, 5169, 33, 271, 4468, 57, 3, 9, 3, 8689, 563, 23655, 15, 26, 12273, 12, 17741, 48, 5447, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 382, 210, 32, 1021, 412, 5, 134, 5, 5169, 130, 4792, 30, 604, 932, 3, 3914, 535, 58, 2163, 42, 150, 58, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2163, 1], [465, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 1}.
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 35 of the evaluation set: {'input_ids': [[37, 1593, 77, 41, 7152, 84, 8, 564, 1473, 19, 3, 9942, 61, 2127, 8, 24672, 11814, 11, 1857, 6601, 358, 24, 66, 8697, 3, 24805, 7, 3010, 130, 12, 1130, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 2247, 77, 4804, 29034, 47, 8, 166, 2830, 26423, 535, 58, 2163, 42, 150, 58, 1], [37, 1593, 77, 41, 7152, 84, 8, 564, 1473, 19, 3, 9942, 61, 2127, 8, 24672, 11814, 11, 1857, 6601, 358, 24, 66, 8697, 3, 24805, 7, 3010, 130, 12, 1130, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 2247, 77, 4804, 29034, 47, 8, 166, 2830, 26423, 535, 58, 2163, 42, 150, 58, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2163, 1], [465, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 1}.
01/16/2023 10:44:37 - INFO - tools.logger -   Sample 31 of the evaluation set: {'input_ids': [[6964, 12551, 11, 24831, 19549, 4243, 26, 6, 14032, 44, 8, 24711, 12154, 496, 6, 20, 17, 106, 920, 13178, 6417, 7, 11, 2946, 1472, 28, 2538, 8765, 7, 6, 3, 9, 18371, 11, 3, 9, 4772, 18698, 447, 609, 8765, 30, 1186, 16047, 5247, 5, 328, 4792, 3, 9, 3145, 11, 586, 481, 11, 21372, 1902, 717, 274, 3, 25345, 12259, 5, 37, 28882, 19170, 8, 684, 114, 150, 119, 5, 94, 47, 8, 6025, 496, 12710, 16, 797, 892, 44, 24, 97, 6, 11, 34, 764, 16, 8, 7178, 13, 3, 9, 985, 18, 26, 32, 1847, 717, 5, 94, 1944, 91, 30, 619, 4390, 6, 7533, 57, 4040, 5, 275, 34, 7283, 8, 9674, 11203, 13, 3, 9, 3, 29720, 797, 800, 10, 24, 2061, 16, 8, 16432, 7, 11, 8, 18137, 130, 43, 29, 7, 13, 3065, 11, 1455, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 2368, 7609, 130, 4792, 57, 192, 481, 16, 5247, 535, 58, 2163, 42, 150, 58, 1], [6964, 12551, 11, 24831, 19549, 4243, 26, 6, 14032, 44, 8, 24711, 12154, 496, 6, 20, 17, 106, 920, 13178, 6417, 7, 11, 2946, 1472, 28, 2538, 8765, 7, 6, 3, 9, 18371, 11, 3, 9, 4772, 18698, 447, 609, 8765, 30, 1186, 16047, 5247, 5, 328, 4792, 3, 9, 3145, 11, 586, 481, 11, 21372, 1902, 717, 274, 3, 25345, 12259, 5, 37, 28882, 19170, 8, 684, 114, 150, 119, 5, 94, 47, 8, 6025, 496, 12710, 16, 797, 892, 44, 24, 97, 6, 11, 34, 764, 16, 8, 7178, 13, 3, 9, 985, 18, 26, 32, 1847, 717, 5, 94, 1944, 91, 30, 619, 4390, 6, 7533, 57, 4040, 5, 275, 34, 7283, 8, 9674, 11203, 13, 3, 9, 3, 29720, 797, 800, 10, 24, 2061, 16, 8, 16432, 7, 11, 8, 18137, 130, 43, 29, 7, 13, 3065, 11, 1455, 5, 11860, 10, 3520, 48, 3, 18531, 24, 96, 2368, 7609, 130, 4792, 57, 192, 481, 16, 5247, 535, 58, 2163, 42, 150, 58, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[2163, 1], [465, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 0}.
01/16/2023 10:44:37 - INFO - tools.logger -   Prepare tensorboard writer...
01/16/2023 10:44:37 - INFO - tools.logger -   Number of parameters: 222903552
01/16/2023 10:44:37 - INFO - tools.logger -   Number of tunable params: 222903552, tunable ratio is 1.0000
01/16/2023 10:44:38 - INFO - tools.logger -   ***** Running training *****
01/16/2023 10:44:38 - INFO - tools.logger -     Num examples = 100
01/16/2023 10:44:38 - INFO - tools.logger -     Num Epochs = 200
01/16/2023 10:44:38 - INFO - tools.logger -     Instantaneous batch size per device = 1
01/16/2023 10:44:38 - INFO - tools.logger -     Gradient Accumulation steps = 4
01/16/2023 10:44:38 - INFO - tools.logger -     Total optimization steps = 5000
01/16/2023 10:44:38 - INFO - tools.logger -   ******************** EPOCH 0 ********************
EPOCH 0:   0% 0/100 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
01/16/2023 10:44:40 - INFO - tools.logger -   TRAIN--> EPOCH 0; STEP 0; LR 2e-08; LOSS 2.163115978240967
EPOCH 0:   1% 1/100 [00:01<02:51,  1.73s/it]EPOCH 0:   2% 2/100 [00:02<01:33,  1.05it/s]EPOCH 0:   3% 3/100 [00:02<01:07,  1.44it/s]EPOCH 0:   4% 4/100 [00:02<00:55,  1.74it/s]EPOCH 0:   5% 5/100 [00:03<00:50,  1.89it/s]EPOCH 0:   6% 6/100 [00:03<00:45,  2.07it/s]EPOCH 0:   7% 7/100 [00:04<00:44,  2.07it/s]EPOCH 0:   8% 8/100 [00:04<00:41,  2.20it/s]EPOCH 0:   9% 9/100 [00:05<00:50,  1.81it/s]EPOCH 0:  10% 10/100 [00:05<00:45,  1.98it/s]EPOCH 0:  11% 11/100 [00:06<00:42,  2.11it/s]EPOCH 0:  12% 12/100 [00:06<00:41,  2.13it/s]EPOCH 0:  13% 13/100 [00:07<00:40,  2.15it/s]EPOCH 0:  14% 14/100 [00:07<00:39,  2.18it/s]EPOCH 0:  15% 15/100 [00:07<00:37,  2.27it/s]EPOCH 0:  16% 16/100 [00:08<00:36,  2.31it/s]EPOCH 0:  17% 17/100 [00:09<00:43,  1.90it/s]EPOCH 0:  18% 18/100 [00:09<00:39,  2.06it/s]EPOCH 0:  19% 19/100 [00:09<00:37,  2.16it/s]EPOCH 0:  20% 20/100 [00:10<00:35,  2.26it/s]01/16/2023 10:44:49 - INFO - tools.logger -   TRAIN--> EPOCH 0; STEP 20; LR 1.2000000000000002e-07; LOSS 1.8351227045059204
EPOCH 0:  21% 21/100 [00:10<00:35,  2.22it/s]EPOCH 0:  22% 22/100 [00:11<00:33,  2.32it/s]EPOCH 0:  23% 23/100 [00:11<00:34,  2.24it/s]EPOCH 0:  24% 24/100 [00:12<00:32,  2.31it/s]EPOCH 0:  25% 25/100 [00:12<00:40,  1.87it/s]EPOCH 0:  26% 26/100 [00:13<00:36,  2.04it/s]EPOCH 0:  27% 27/100 [00:13<00:33,  2.15it/s]EPOCH 0:  28% 28/100 [00:14<00:32,  2.20it/s]EPOCH 0:  29% 29/100 [00:14<00:32,  2.17it/s]EPOCH 0:  30% 30/100 [00:14<00:30,  2.28it/s]EPOCH 0:  31% 31/100 [00:15<00:29,  2.33it/s]EPOCH 0:  32% 32/100 [00:15<00:28,  2.41it/s]EPOCH 0:  33% 33/100 [00:16<00:31,  2.12it/s]EPOCH 0:  34% 34/100 [00:16<00:29,  2.24it/s]EPOCH 0:  35% 35/100 [00:17<00:28,  2.32it/s]EPOCH 0:  36% 36/100 [00:17<00:26,  2.40it/s]EPOCH 0:  37% 37/100 [00:18<00:31,  2.00it/s]EPOCH 0:  38% 38/100 [00:18<00:29,  2.12it/s]EPOCH 0:  39% 39/100 [00:18<00:27,  2.25it/s]EPOCH 0:  40% 40/100 [00:19<00:25,  2.35it/s]01/16/2023 10:44:58 - INFO - tools.logger -   TRAIN--> EPOCH 0; STEP 40; LR 2.2e-07; LOSS 1.6710089445114136
EPOCH 0:  41% 41/100 [00:19<00:25,  2.33it/s]EPOCH 0:  42% 42/100 [00:20<00:24,  2.38it/s]EPOCH 0:  43% 43/100 [00:20<00:23,  2.41it/s]EPOCH 0:  44% 44/100 [00:20<00:22,  2.45it/s]EPOCH 0:  45% 45/100 [00:21<00:26,  2.06it/s]EPOCH 0:  46% 46/100 [00:22<00:24,  2.16it/s]EPOCH 0:  47% 47/100 [00:22<00:23,  2.26it/s]EPOCH 0:  48% 48/100 [00:22<00:22,  2.33it/s]EPOCH 0:  49% 49/100 [00:23<00:23,  2.19it/s]EPOCH 0:  50% 50/100 [00:23<00:21,  2.30it/s]EPOCH 0:  51% 51/100 [00:24<00:20,  2.36it/s]EPOCH 0:  52% 52/100 [00:24<00:20,  2.40it/s]EPOCH 0:  53% 53/100 [00:25<00:20,  2.31it/s]EPOCH 0:  54% 54/100 [00:25<00:19,  2.39it/s]EPOCH 0:  55% 55/100 [00:25<00:18,  2.42it/s]EPOCH 0:  56% 56/100 [00:26<00:19,  2.29it/s]EPOCH 0:  57% 57/100 [00:26<00:19,  2.26it/s]EPOCH 0:  58% 58/100 [00:27<00:18,  2.30it/s]EPOCH 0:  59% 59/100 [00:27<00:17,  2.35it/s]EPOCH 0:  60% 60/100 [00:27<00:16,  2.39it/s]01/16/2023 10:45:07 - INFO - tools.logger -   TRAIN--> EPOCH 0; STEP 60; LR 3.2e-07; LOSS 1.8498785495758057
EPOCH 0:  61% 61/100 [00:28<00:19,  1.99it/s]EPOCH 0:  62% 62/100 [00:29<00:18,  2.08it/s]EPOCH 0:  63% 63/100 [00:29<00:16,  2.20it/s]EPOCH 0:  64% 64/100 [00:29<00:16,  2.16it/s]EPOCH 0:  65% 65/100 [00:30<00:19,  1.83it/s]EPOCH 0:  66% 66/100 [00:31<00:16,  2.00it/s]EPOCH 0:  67% 67/100 [00:31<00:15,  2.16it/s]