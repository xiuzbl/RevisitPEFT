{
    "adapter_type": "normal",
    "adapter_reduction_factor": 16,
    "adapter_non_linearity": "relu",
    "normal_adapter_residual": true,
    "trainable_param_names": ".*layer_norm.*|.*adapter.*",
    "model_modifier": "adapters",
    "add_compacter_in_attention": true,
    "compacter_add_compacter_in_self_attention": false,
    "compacter_add_compacter_in_cross_attention": false

}