Warning: Permanently added '[10.184.185.27]:49439' (ECDSA) to the list of known hosts.
[2023-01-23 15:13:29,272] [INFO] [runner.py:288:main] Using IP address of 10.184.185.27 for node worker-0
[2023-01-23 15:13:29,272] [INFO] [runner.py:355:main] cmd = /azure/yingxiu/ENVS/uniskg/bin/python -u -m deepspeed.launcher.launch --world_info=eyJ3b3JrZXItMCI6IFswLCAxLCAyLCAzLCA0LCA1LCA2LCA3XX0= --master_addr=10.184.185.27 --master_port=29500 train.py --deepspeed deepspeed/ds_config_zero2.json --seed 42 --cfg=Salesforce/T5_base_adapter_spider_with_cell_value.cfg --run_name=ds_T5-base_adapter_spider --logging_strategy steps --logging_first_step true --logging_steps 4 --evaluation_strategy steps --eval_steps 500 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 500 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 16 --num_train_epochs 50 --adafactor false --learning_rate 5e-5 --do_train --do_eval --do_predict --predict_with_generate --output_dir output/ds_T5-base_adapter_spider --overwrite_output_dir --per_device_train_batch_size=1 --per_device_eval_batch_size 1 --generation_num_beams=1 --generation_max_length=256 --input_max_length=512 --ddp_find_unused_parameters true
[2023-01-23 15:13:38,482] [INFO] [launch.py:73:main] 0 NCCL_VERSION=2.7.8
[2023-01-23 15:13:38,483] [INFO] [launch.py:80:main] WORLD INFO DICT: {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]}
[2023-01-23 15:13:38,483] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=8, node_rank=0
[2023-01-23 15:13:38,483] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'worker-0': [0, 1, 2, 3, 4, 5, 6, 7]})
[2023-01-23 15:13:38,483] [INFO] [launch.py:100:main] dist_world_size=8
[2023-01-23 15:13:38,483] [INFO] [launch.py:102:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
INFO:filelock:Lock 140332624983152 acquired on .lock
INFO:filelock:Lock 140332624983152 released on .lock
[2023-01-23 15:16:21,135] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
INFO:filelock:Lock 140556914824080 acquired on .lock
INFO:filelock:Lock 140556914824080 released on .lock
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
[2023-01-23 15:16:35,067] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
INFO:filelock:Lock 139856815809536 acquired on .lock
INFO:filelock:Lock 139856815809536 released on .lock
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
[2023-01-23 15:16:38,432] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
INFO:filelock:Lock 139760787567672 acquired on .lock
INFO:filelock:Lock 139760787567672 released on .lock
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
INFO:filelock:Lock 139802343097400 acquired on .lock
[W Context.cpp:69] Warning: torch.set_deterministic is in beta, and its design and  functionality may change in the future. (function operator())
INFO:filelock:Lock 139802343097400 released on .lock
[2023-01-23 15:16:41,489] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
INFO:filelock:Lock 140229375177784 acquired on .lock
[2023-01-23 15:16:42,928] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
INFO:filelock:Lock 140229375177784 released on .lock
INFO:filelock:Lock 140395304105016 acquired on .lock
INFO:filelock:Lock 140395304105016 released on .lock
INFO:filelock:Lock 140294636508160 acquired on .lock
INFO:filelock:Lock 140294636508160 released on .lock
[2023-01-23 15:16:44,905] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
[2023-01-23 15:16:45,460] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
[2023-01-23 15:16:46,200] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl
<utils.configue.Args object at 0x7f89ad833e10>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7fa1b7ae3e80>
<utils.configue.Args object at 0x7f26406ace48>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7f98df641e48>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7fb04fa5beb8>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7f1c9383be48>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7f32ef3e6e80>
task_args.bert.location: t5-base
<utils.configue.Args object at 0x7fd5f0659e48>
task_args.bert.location: t5-base
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
wandb: Currently logged in as: niesheng (use `wandb login --relogin` to force relogin)
prefix-tuning sequence length is 10.
adapter is used.
prefix-tuning sequence length is 10.
adapter is used.
prefix-tuning sequence length is 10.
prefix-tuning sequence length is 10.adapter is used.

adapter is used.
prefix-tuning sequence length is 10.
adapter is used.
prefix-tuning sequence length is 10.
adapter is used.
prefix-tuning sequence length is 10.
adapter is used.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.7.layer.1.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.3.layer.1.adapter.adapter_up.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_up.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['decoder.block.2.layer.2.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
Trainer build successfully.
wandb: Tracking run with wandb version 0.12.1
wandb: Syncing run ds_T5-base_adapter_spider
wandb:  View project at https://wandb.ai/niesheng/uniskg
wandb:  View run at https://wandb.ai/niesheng/uniskg/runs/2cupjgc5
wandb: Run data is saved locally in /azure/yingxiu/Yingxiu_Intern/UnifiedSKG/wandb/run-20230123_151703-2cupjgc5
wandb: Run `wandb offline` to turn off syncing.

task_args.bert.location: t5-base
WARNING:datasets.builder:Reusing dataset spider (./data/spider/spider/1.0.0/b92106441d4d0ce75e60b8a70d2c86b66c0eec351ae3af9eb191474d3eefa97d)
prefix-tuning sequence length is 10.
adapter is used.
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-base and are newly initialized: ['encoder.block.0.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_up.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_down.0.bias', 'encoder.block.6.layer.1.adapter.adapter_up.weight', 'decoder.block.5.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_up.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.6.layer.2.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_down.0.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_up.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_up.bias', 'decoder.block.10.layer.2.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_up.weight', 'encoder.block.4.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.weight', 'decoder.block.9.layer.2.adapter.adapter_down.0.weight', 'decoder.block.5.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.bias', 'decoder.block.4.layer.2.adapter.adapter_up.bias', 'decoder.block.8.layer.2.adapter.adapter_down.0.bias', 'decoder.block.6.layer.2.adapter.adapter_down.1.weight', 'encoder.block.10.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.5.layer.2.adapter.adapter_down.1.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.0.layer.1.adapter.adapter_up.weight', 'encoder.block.0.layer.1.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.bias', 'encoder.block.10.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.4.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.1.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.7.layer.1.adapter.adapter_down.1.weight', 'encoder.block.0.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_up.bias', 'decoder.block.5.layer.2.adapter.adapter_down.0.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_up.weight', 'encoder.block.3.layer.1.adapter.adapter_down.1.bias', 'encoder.block.11.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_down.1.bias', 'encoder.block.1.layer.1.adapter.adapter_up.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.bias', 'decoder.block.11.layer.2.adapter.adapter_down.1.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_down.1.bias', 'encoder.block.6.layer.1.adapter.adapter_down.0.bias', 'encoder.block.11.layer.1.adapter.adapter_up.weight', 'encoder.block.1.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_down.1.weight', 'decoder.block.7.layer.2.adapter.adapter_down.0.weight', 'decoder.block.11.layer.2.adapter.adapter_up.weight', 'encoder.block.11.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.2.layer.1.adapter.adapter_down.0.bias', 'encoder.block.7.layer.1.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_down.1.weight', 'encoder.block.4.layer.1.adapter.adapter_down.1.weight', 'decoder.block.5.layer.2.adapter.adapter_up.bias', 'encoder.block.6.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_up.bias', 'decoder.block.7.layer.2.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.bias', 'encoder.block.3.layer.1.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_up.bias', 'decoder.block.2.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.weight', 'decoder.block.8.layer.2.adapter.adapter_up.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.10.layer.1.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_down.1.weight', 'decoder.block.4.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.3.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.2.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_down.1.weight', 'decoder.block.2.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.weight', 'encoder.block.11.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_up.bias', 'decoder.block.1.layer.2.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_down.0.weight', 'decoder.block.4.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.bias', 'decoder.block.1.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_up.weight', 'encoder.block.5.layer.1.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_down.1.bias', 'decoder.block.8.layer.2.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_norm_before.bias', 'encoder.block.4.layer.1.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_down.1.bias', 'encoder.block.0.layer.1.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.bias', 'decoder.block.3.layer.2.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_down.0.bias', 'encoder.block.1.layer.1.adapter.adapter_down.1.bias', 'decoder.block.4.layer.2.adapter.adapter_down.0.weight', 'encoder.block.7.layer.1.adapter.adapter_up.bias', 'encoder.block.4.layer.1.adapter.adapter_up.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_up.weight', 'decoder.block.6.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.6.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_down.1.bias', 'encoder.block.3.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.bias', 'decoder.block.10.layer.2.adapter.adapter_down.1.bias', 'encoder.block.7.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_down.1.weight', 'encoder.block.7.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.6.layer.2.adapter.adapter_down.0.bias', 'decoder.block.2.layer.2.adapter.adapter_down.0.bias', 'decoder.block.0.layer.2.adapter.adapter_up.weight', 'encoder.block.10.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.8.layer.2.adapter.adapter_down.0.weight', 'encoder.block.6.layer.1.adapter.adapter_up.bias', 'decoder.block.0.layer.2.adapter.adapter_down.1.weight', 'encoder.block.9.layer.1.adapter.adapter_down.0.bias', 'decoder.block.9.layer.2.adapter.adapter_down.0.bias', 'decoder.block.11.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.0.weight', 'encoder.block.4.layer.1.adapter.adapter_up.bias', 'encoder.block.5.layer.1.adapter.adapter_up.bias', 'encoder.block.11.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.3.layer.1.adapter.adapter_down.0.weight', 'decoder.block.3.layer.2.adapter.adapter_norm_before.bias', 'decoder.block.7.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.3.layer.2.adapter.adapter_down.1.weight', 'encoder.block.8.layer.1.adapter.adapter_up.bias', 'decoder.block.6.layer.2.adapter.adapter_up.weight', 'decoder.block.4.layer.2.adapter.adapter_down.1.bias', 'encoder.block.9.layer.1.adapter.adapter_up.weight', 'decoder.block.8.layer.2.adapter.adapter_norm_before.weight', 'encoder.block.2.layer.1.adapter.adapter_down.0.weight', 'decoder.block.10.layer.2.adapter.adapter_norm_before.weight', 'decoder.block.9.layer.2.adapter.adapter_up.bias', 'encoder.block.1.layer.1.adapter.adapter_down.0.weight', 'decoder.block.0.layer.2.adapter.adapter_up.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.bias', 'encoder.block.8.layer.1.adapter.adapter_down.0.weight', 'encoder.block.2.layer.1.adapter.adapter_up.weight', 'encoder.block.8.layer.1.adapter.adapter_down.1.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.bias', 'decoder.block.1.layer.2.adapter.adapter_down.1.weight', 'decoder.block.11.layer.2.adapter.adapter_up.bias', 'decoder.block.3.layer.2.adapter.adapter_down.1.bias', 'encoder.block.10.layer.1.adapter.adapter_down.0.weight', 'encoder.block.5.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.11.layer.2.adapter.adapter_down.1.bias', 'encoder.block.8.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.0.layer.2.adapter.adapter_norm_before.bias', 'encoder.block.9.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.10.layer.2.adapter.adapter_down.0.weight', 'encoder.block.9.layer.1.adapter.adapter_norm_before.bias', 'decoder.block.9.layer.2.adapter.adapter_down.1.bias', 'encoder.block.4.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.8.layer.1.adapter.adapter_norm_before.weight', 'encoder.block.9.layer.1.adapter.adapter_down.1.weight', 'encoder.block.2.layer.1.adapter.adapter_norm_before.weight', 'decoder.block.1.layer.2.adapter.adapter_up.weight', 'decoder.block.0.layer.2.adapter.adapter_down.0.weight', 'decoder.block.6.layer.2.adapter.adapter_down.1.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
INFO:__main__:Number of parameters: 237103872
INFO:__main__:Number of tunable params: 14220288, tunable ratio is 0.0600
Trainer build successfully.
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[2023-01-23 15:17:50,623] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.5.6, git-hash=unknown, git-branch=unknown
[2023-01-23 15:17:51,015] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed groups
[2023-01-23 15:17:51,015] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1
[2023-01-23 15:17:51,020] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0, 1, 2, 3, 4, 5, 6, 7]
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [1]
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [2]
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [3]
[2023-01-23 15:17:51,021] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [4]
[2023-01-23 15:17:51,022] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [5]
[2023-01-23 15:17:51,022] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [6]
[2023-01-23 15:17:51,022] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [7]
[2023-01-23 15:17:54,650] [INFO] [engine.py:208:__init__] DeepSpeed Flops Profiler Enabled: False
[2023-01-23 15:17:54,650] [INFO] [engine.py:871:_configure_optimizer] Removing param_group that has no 'params' in the client Optimizer
[2023-01-23 15:17:54,651] [INFO] [engine.py:876:_configure_optimizer] Using client Optimizer as basic optimizer
[2023-01-23 15:17:54,674] [INFO] [engine.py:893:_configure_optimizer] DeepSpeed Basic Optimizer = AdamW
[2023-01-23 15:17:54,674] [INFO] [utils.py:44:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'transformers.optimization.AdamW'>
[2023-01-23 15:17:54,674] [WARNING] [engine.py:903:_configure_optimizer] **** You are using ZeRO with an untested optimizer, proceed with caution *****
[2023-01-23 15:17:54,674] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2023-01-23 15:17:54,674] [INFO] [stage2.py:112:__init__] Reduce bucket size 200000000.0
[2023-01-23 15:17:54,674] [INFO] [stage2.py:113:__init__] Allgather bucket size 200000000.0
[2023-01-23 15:17:54,674] [INFO] [stage2.py:114:__init__] CPU Offload: True
[2023-01-23 15:17:54,674] [INFO] [stage2.py:115:__init__] Round robin gradient partitioning: False
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Emitting ninja build file /home/v-yingxzhao/.cache/torch_extensions/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 5.220099925994873 seconds
Loading extension module utils...
Time to load utils op: 2.807408094406128 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 2.0052196979522705 seconds
Time to load utils op: 1.9047212600708008 seconds
Loading extension module utils...
Loading extension module utils...
Time to load utils op: 1.905961275100708 seconds
Time to load utils op: 2.206392765045166 seconds
Loading extension module utils...
Time to load utils op: 2.506075620651245 seconds
Loading extension module utils...
Time to load utils op: 3.4078409671783447 seconds
Rank: 7 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 1 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 5 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 0 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 6 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 4 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 3 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Rank: 2 partition count [8, 8] and sizes[(29629824, False), (8160, False)] 
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.035556793212890625 seconds
[2023-01-23 15:18:33,119] [INFO] [utils.py:806:see_memory_usage] Before initializing optimizer states
[2023-01-23 15:18:33,120] [INFO] [utils.py:811:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 1.82 GB         Max_CA 2 GB 
[2023-01-23 15:18:33,120] [INFO] [utils.py:816:see_memory_usage] CPU Virtual Memory:  used = 85.38 GB, percent = 5.7%
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...No modifications detected for re-loaded extension module utils, skipping build step...

Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...Loading extension module utils...Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.07916259765625 seconds

No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.06109309196472168 seconds
No modifications detected for re-loaded extension module utils, skipping build step...Time to load utils op: 0.017516374588012695 seconds


Loading extension module utils...
Time to load utils op: 0.024158239364624023 seconds
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.01637744903564453 seconds
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.014640092849731445 seconds
[2023-01-23 15:18:34,721] [INFO] [utils.py:806:see_memory_usage] After initializing optimizer states
[2023-01-23 15:18:34,722] [INFO] [utils.py:811:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 1.82 GB         Max_CA 2 GB 
[2023-01-23 15:18:34,722] [INFO] [utils.py:816:see_memory_usage] CPU Virtual Memory:  used = 85.23 GB, percent = 5.6%
[2023-01-23 15:18:34,723] [INFO] [stage2.py:477:__init__] optimizer state initialized
[2023-01-23 15:18:35,158] [INFO] [utils.py:806:see_memory_usage] After initializing ZeRO optimizer
[2023-01-23 15:18:35,159] [INFO] [utils.py:811:see_memory_usage] MA 0.98 GB         Max_MA 0.98 GB         CA 1.82 GB         Max_CA 2 GB 
[2023-01-23 15:18:35,159] [INFO] [utils.py:816:see_memory_usage] CPU Virtual Memory:  used = 85.22 GB, percent = 5.6%
[2023-01-23 15:18:35,159] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2023-01-23 15:18:35,159] [INFO] [engine.py:605:_configure_lr_scheduler] DeepSpeed using client LR scheduler
[2023-01-23 15:18:35,159] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7fa874d03198>
[2023-01-23 15:18:35,159] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05, 5e-05], mom=[(0.9, 0.999), (0.9, 0.999)]
[2023-01-23 15:18:35,161] [INFO] [config.py:958:print] DeepSpeedEngine configuration:
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   allreduce_always_fp32 ........ False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   amp_enabled .................. False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   amp_params ................... False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   bfloat16_enabled ............. False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   checkpoint_tag_validation_enabled  True
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   checkpoint_tag_validation_fail  False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   curriculum_enabled ........... False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   curriculum_params ............ False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   dataloader_drop_last ......... False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   disable_allgather ............ False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   dump_state ................... False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   dynamic_loss_scale_args ...... None
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_enabled ........... False
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_gas_boundary_resolution  1
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_layer_num ......... 0
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_max_iter .......... 100
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_stability ......... 1e-06
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_tol ............... 0.01
[2023-01-23 15:18:35,162] [INFO] [config.py:962:print]   eigenvalue_verbose ........... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   elasticity_enabled ........... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   fp16_enabled ................. False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   fp16_master_weights_and_gradients  False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   fp16_mixed_quantize .......... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   global_rank .................. 0
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   gradient_accumulation_steps .. 16
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   gradient_clipping ............ 1.0
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   gradient_predivide_factor .... 1.0
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   initial_dynamic_scale ........ 4294967296
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   loss_scale ................... 0
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   memory_breakdown ............. False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   optimizer_legacy_fusion ...... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   optimizer_name ............... None
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   optimizer_params ............. None
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   pld_enabled .................. False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   pld_params ................... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   prescale_gradients ........... False
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   quantize_change_rate ......... 0.001
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   quantize_groups .............. 1
[2023-01-23 15:18:35,163] [INFO] [config.py:962:print]   quantize_offset .............. 1000
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_period .............. 1000
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_rounding ............ 0
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_start_bits .......... 16
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_target_bits ......... 8
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_training_enabled .... False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_type ................ 0
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   quantize_verbose ............. False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   scheduler_name ............... None
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   scheduler_params ............. None
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   sparse_attention ............. None
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   sparse_gradients_enabled ..... False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   steps_per_print .............. 2000
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   tensorboard_enabled .......... False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   tensorboard_job_name ......... DeepSpeedJobName
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   tensorboard_output_path ...... 
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   train_batch_size ............. 128
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   train_micro_batch_size_per_gpu  1
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   use_quantizer_kernel ......... False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   wall_clock_breakdown ......... False
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   world_size ................... 8
[2023-01-23 15:18:35,164] [INFO] [config.py:962:print]   zero_allow_untested_optimizer  True
[2023-01-23 15:18:35,165] [INFO] [config.py:962:print]   zero_config .................. {
    "stage": 2, 
    "contiguous_gradients": true, 
    "reduce_scatter": true, 
    "reduce_bucket_size": 2.000000e+08, 
    "allgather_partitions": true, 
    "allgather_bucket_size": 2.000000e+08, 
    "overlap_comm": true, 
    "load_from_fp32_weights": true, 
    "elastic_checkpoint": true, 
    "offload_param": null, 
    "offload_optimizer": {
        "device": "cpu", 
        "nvme_path": null, 
        "buffer_count": 4, 
        "pin_memory": true, 
        "pipeline_read": false, 
        "pipeline_write": false, 
        "fast_init": false, 
        "pipeline": false
    }, 
    "sub_group_size": 1.000000e+09, 
    "prefetch_bucket_size": 5.000000e+07, 
    "param_persistence_threshold": 1.000000e+05, 
    "max_live_parameters": 1.000000e+09, 
    "max_reuse_distance": 1.000000e+09, 
    "gather_fp16_weights_on_model_save": false, 
    "ignore_unused_parameters": true, 
    "round_robin_gradients": false, 
    "legacy_stage1": false
}
[2023-01-23 15:18:35,165] [INFO] [config.py:962:print]   zero_enabled ................. True
[2023-01-23 15:18:35,165] [INFO] [config.py:962:print]   zero_optimization_stage ...... 2
[2023-01-23 15:18:35,165] [INFO] [config.py:969:print]   json = {
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "zero_optimization": {
        "stage": 2, 
        "offload_optimizer": {
            "device": "cpu", 
            "pin_memory": true
        }, 
        "allgather_partitions": true, 
        "allgather_bucket_size": 2.000000e+08, 
        "overlap_comm": true, 
        "reduce_scatter": true, 
        "reduce_bucket_size": 2.000000e+08, 
        "contiguous_gradients": true
    }, 
    "gradient_accumulation_steps": 16, 
    "gradient_clipping": 1.0, 
    "steps_per_print": 2.000000e+03, 
    "train_batch_size": 128, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "zero_allow_untested_optimizer": true
}
Using /home/v-yingxzhao/.cache/torch_extensions as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006468296051025391 seconds
***** Running training *****
  Num examples = 7000
  Num Epochs = 50
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 128
  Gradient Accumulation steps = 16
  Total optimization steps = 2700
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
  0%|                                                                                                                                         | 0/2700 [00:00<?, ?it/s]  0%|                                                                                                                              | 1/2700 [00:55<41:17:58, 55.09s/it]                                                                                                                                                                         0%|                                                                                                                              | 1/2700 [00:55<41:17:58, 55.09s/it]{'loss': 4.6843, 'learning_rate': 4.998148148148148e-05, 'epoch': 0.02}
  0%|                                                                                                                              | 2/2700 [01:51<41:50:16, 55.83s/it]  0%|▏                                                                                                                             | 3/2700 [02:48<42:11:30, 56.32s/it]  0%|▏                                                                                                                             | 4/2700 [03:45<42:28:44, 56.72s/it]                                                                                                                                                                       {'loss': 4.6304, 'learning_rate': 4.9925925925925926e-05, 'epoch': 0.07}
  0%|▏                                                                                                                             | 4/2700 [03:45<42:28:44, 56.72s/it]  0%|▏                                                                                                                             | 5/2700 [04:42<42:22:47, 56.61s/it]  0%|▎                                                                                                                             | 6/2700 [05:38<42:25:43, 56.70s/it]  0%|▎                                                                                                                             | 7/2700 [06:30<41:15:53, 55.16s/it]  0%|▎                                                                                                                             | 8/2700 [07:21<40:08:49, 53.69s/it]                                                                                                                                                                       {'loss': 4.5936, 'learning_rate': 4.9851851851851855e-05, 'epoch': 0.15}
  0%|▎                                                                                                                             | 8/2700 [07:21<40:08:49, 53.69s/it]  0%|▍                                                                                                                             | 9/2700 [08:11<39:21:30, 52.65s/it]  0%|▍                                                                                                                            | 10/2700 [09:02<38:50:44, 51.99s/it]  0%|▌                                                                                                                            | 11/2700 [09:55<38:59:02, 52.19s/it]  0%|▌                                                                                                                            | 12/2700 [10:45<38:37:41, 51.73s/it]                                                                                                                                                                       {'loss': 4.669, 'learning_rate': 4.977777777777778e-05, 'epoch': 0.22}
  0%|▌                                                                                                                            | 12/2700 [10:45<38:37:41, 51.73s/it]  0%|▌                                                                                                                            | 13/2700 [11:36<38:17:22, 51.30s/it]  1%|▋                                                                                                                            | 14/2700 [12:29<38:46:21, 51.97s/it]  1%|▋                                                                                                                            | 15/2700 [13:25<39:39:07, 53.16s/it]  1%|▋                                                                                                                            | 16/2700 [14:18<39:34:35, 53.08s/it]                                                                                                                                                                       {'loss': 4.2631, 'learning_rate': 4.970370370370371e-05, 'epoch': 0.29}  1%|▋                                                                                                                            | 16/2700 [14:18<39:34:35, 53.08s/it]
  1%|▊                                                                                                                            | 17/2700 [15:08<38:55:20, 52.23s/it]  1%|▊                                                                                                                            | 18/2700 [15:58<38:20:28, 51.46s/it]  1%|▉                                                                                                                            | 19/2700 [16:48<38:03:46, 51.11s/it]  1%|▉                                                                                                                            | 20/2700 [17:38<37:49:41, 50.81s/it]                                                                                                                                                                       {'loss': 4.6689, 'learning_rate': 4.962962962962963e-05, 'epoch': 0.37}
  1%|▉                                                                                                                            | 20/2700 [17:38<37:49:41, 50.81s/it]  1%|▉                                                                                                                            | 21/2700 [18:29<37:53:42, 50.92s/it]  1%|█                                                                                                                            | 22/2700 [19:20<37:53:25, 50.94s/it]  1%|█                                                                                                                            | 23/2700 [20:11<37:45:19, 50.77s/it]Traceback (most recent call last):
Traceback (most recent call last):
Traceback (most recent call last):
[2023-01-23 15:39:05,388] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32112  File "train.py", line 230, in <module>
Traceback (most recent call last):
  File "train.py", line 230, in <module>
Traceback (most recent call last):
  File "train.py", line 230, in <module>
  File "train.py", line 230, in <module>
  File "train.py", line 230, in <module>

Traceback (most recent call last):
    main()Traceback (most recent call last):
    main()  File "train.py", line 230, in <module>

  File "train.py", line 189, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 1287, in train
    main()
  File "train.py", line 189, in main
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 1287, in train
      File "train.py", line 230, in <module>
[2023-01-23 15:39:05,390] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32113
    self.current_flos += float(self.floating_point_ops(inputs))    self.current_flos += float(self.floating_point_ops(inputs))    main()
  File "train.py", line 189, in main

main()
    
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 2458, in floating_point_ops
[2023-01-23 15:39:05,392] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32114  File "train.py", line 189, in main
main()  File "train.py", line 189, in main
    
    
main()  File "train.py", line 189, in main
        [2023-01-23 15:39:05,393] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32115    
train_result = trainer.train(resume_from_checkpoint=checkpoint)train_result = trainer.train(resume_from_checkpoint=checkpoint)
    return self.model.floating_point_ops(inputs)
  File "train.py", line 189, in main

  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 1287, in train
train_result = trainer.train(resume_from_checkpoint=checkpoint)[2023-01-23 15:39:05,394] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32116  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/modeling_utils.py", line 410, in floating_point_ops


  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 1287, in train
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
[2023-01-23 15:39:05,395] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32117    self.current_flos += float(self.floating_point_ops(inputs))  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 1287, in train


  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 2458, in floating_point_ops
[2023-01-23 15:39:05,395] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32118
    self.current_flos += float(self.floating_point_ops(inputs))
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/transformers/trainer.py", line 2458, in floating_point_ops
[2023-01-23 15:39:05,396] [INFO] [launch.py:131:sigkill_handler] Killing subprocess 32119
[2023-01-23 15:39:05,397] [INFO] [launch.py:139:sigkill_handler] Main process received SIGINT, exiting
Traceback (most recent call last):
  File "/azure/yingxiu/ENVS/uniskg/bin/deepspeed", line 6, in <module>
    main()
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/site-packages/deepspeed/launcher/runner.py", line 357, in main
    result.wait()
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/subprocess.py", line 971, in wait
    return self._wait(timeout=timeout)
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/subprocess.py", line 1601, in _wait
    (pid, sts) = self._try_wait(0)
  File "/azure/yingxiu/ENVS/uniskg/lib/python3.7/subprocess.py", line 1559, in _try_wait
    (pid, sts) = os.waitpid(self.pid, wait_flags)
KeyboardInterrupt
/azure/yingxiu/ENVS/uniskg/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
